---
layout: post
title: Why is Softmax called 'Softmax'?
comments: true
author: Hashir Ahmad
---

A week back, I was studying for the finals of a course titled *Machine Learning for Graphs and Sequential Data*. Everything was usual. I was working out the heavy math that could be seen in almost every slide of the course. However, I came across something which might have been overlooked by many people who work or study Machine Learning. And believe me when I say this, I or someone else would have never cared enough to figure this thing on our own. 

It was an optimization problem for the `argmax` function and the instructor said
> Since the argmax function is not differentiable, we use softmax instead

I paused the lecture, went back and heard it again (thanks to online classes ;)). I knew that the argmax is not differentiable for equal arguments. I also knew that softmax transforms an input vector into a probability distribution. What came as a surprise that there was possibly a relation between `argmax` and `softmax`. At that moment, I had to let this thought go away. I could not afford another adventure besides the one where I had to study and pass the course. 

Fast forward today, I searched on Google "relation between argmax and softmax". I was blown away by what I was seeing. It just so happens that the softmax is a smooth (or soft) approximation of the argmax function. But then you ask, why the name softmax? Actually, the name is a misnomer. Quoting the wikipedia [page](https://en.wikipedia.org/wiki/Softmax_function) for softmax:

> The name "softmax" is misleading; the function is not a smooth maximum (a smooth approximation to the maximum function), but is rather a smooth approximation to the arg max function: the function whose value is which index has the maximum. In fact, the term "softmax" is also used for the closely related LogSumExp function, which is a smooth maximum. For this reason, some prefer the more accurate term "softargmax", but the term "softmax" is conventional in machine learning.

In other words, the actual `max` is the `hardmax` output of the vector whereas the `LogSumExp` is a smooth approximation to it. Further, the `softmax` is a smooth approximation of the `argmax`. For this particular reason, the `softmax` should ideally be called `softargmax` and the `LogSumExp` is also called (drum rolls) `RealSoftMax`.

All this information had been there but it seemed that I came across a very recently development in Machine Learning. To confirm this result myself, I fired up a jupyter notebook and played with all of these - `max (or hardmax)`, `argmax`, `softmax` and `LogSumExp`. In the rest of this post, I have provided snippets from the notebook.

{% highlight python %}
import numpy as np
np.set_printoptions(suppress=True) # suppress scientific notation of numbers
{% endhighlight %}

Now, as per the information, the softmax is actually an approximation of the argmax of a vector while the log-sum-exponential is an approximation of the max of a vector. 

Let's create functions for both using numpy:
{% highlight python %}
def softmax(x):
    return np.exp(x)/np.exp(x).sum()

def logsumexp(x):
    return np.log(np.exp(x).sum())
{% endhighlight %}

Now, let's create a simple 1D array. Since the softmax returns a probability distribution over the array, I created a one-hot representation of the argmax instead of the categorical output for better comparison.
{% highlight python %}
x = np.array([0, 1, -4, 2, 7])

x_argmax = np.zeros_like(x)
x_argmax[np.argmax(x)] = 1 
# [0, 0, 0, 0, 1]

x_softmax = softmax(x) 
# [0.00090272 0.00245386 0.00001653 0.00667028 0.98995661]
{% endhighlight %}

As we see above, the softmax is pretty close to the one-hot encoded argmax output. Neat!

Now, for the max and LogSumExp relation:
{% highlight python %}
x_max = np.max(x) 
# 7

x_logsumexp = logsumexp(x)
# 7.01009416490667
{% endhighlight %}

Again, the outputs are pretty close. 

Now that you know it, these relations are actually very helpful. Both softmax and LogSumExp are differentiable while argmax and max are not for equal arguments. This is why we use softmax and LogSumExp extensively in Machine Learning as an approximation.